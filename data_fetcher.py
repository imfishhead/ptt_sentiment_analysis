# data_fetcher.py

import streamlit as st
import pandas as pd
import datetime
import requests
from bs4 import BeautifulSoup
import time
import http.cookiejar

# é€™è£¡æ‡‰è©²æ”¾ç½®ä½ çš„ PTT çˆ¬èŸ²å’Œè³‡æ–™åº«è®€å–é‚è¼¯
# ç‚ºäº†ç¯„ä¾‹ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š

def get_ptt_articles_from_db(board: str, last_time=None) -> pd.DataFrame:
    """
    åªæŠ“æ¯” last_time æ–°çš„æ–‡ç« ï¼Œä¸¦èˆ‡ cache åˆä½µå»é‡ã€‚
    """
    st.write(f"ğŸ” æ­£åœ¨çˆ¬å– PTT {board} çœ‹æ¿éå»ä¸ƒå¤©çš„æ–‡ç« ...")
    base_url = "https://www.ptt.cc"
    url = f"https://www.ptt.cc/bbs/{board}/index.html"
    articles = []
    progress_msg = st.empty()
    info_msg = st.empty()
    warning_msg = st.empty()
    error_msg = st.empty()
    
    # æ›´çœŸå¯¦çš„ç€è¦½å™¨æ¨™é ­
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'Referer': 'https://www.google.com/'
    })
    
    # è¨­å®šæ›´å®Œæ•´çš„ cookies
    session.cookies.set('over18', '1', domain='www.ptt.cc', path='/')
    session.cookies.set('_ga', 'GA1.1.1234567890.1234567890', domain='.ptt.cc', path='/')
    session.cookies.set('_ga_1234567890', 'GS1.1.1234567890.1.1.1234567890.0.0.0', domain='.ptt.cc', path='/')
    
    days_to_scrape = 7
    today = datetime.date.today()
    start = today - datetime.timedelta(days=days_to_scrape-1)
    max_pages = 20
    request_delay = 3.0  # å¢åŠ å»¶é²æ™‚é–“
    page_delay = 2.0     # å¢åŠ é é¢é–“å»¶é²
    current_count = 0
    page = 1
    stop_crawling = False
    
    info_msg.info(f"é–‹å§‹çˆ¬å–ï¼Œç›®æ¨™æ—¥æœŸç¯„åœï¼š{start} ~ {today}")
    info_msg.info(f"ç›®æ¨™ URLï¼š{url}")
    
    # é¦–å…ˆè¨ªå• Googleï¼Œç„¶å¾Œå†è¨ªå• PTTï¼ˆæ¨¡æ“¬çœŸå¯¦ç€è¦½è¡Œç‚ºï¼‰
    try:
        info_msg.info("æ¨¡æ“¬çœŸå¯¦ç€è¦½è¡Œç‚ºï¼šå…ˆè¨ªå• Google...")
        session.get("https://www.google.com", timeout=10)
        time.sleep(2)
        
        info_msg.info("æ¸¬è©¦ PTT é€£ç·š...")
        test_res = session.get("https://www.ptt.cc/bbs/index.html", timeout=15)
        info_msg.info(f"PTT ä¸»é é€£ç·šæ¸¬è©¦ï¼šç‹€æ…‹ç¢¼ {test_res.status_code}")
        
        if test_res.status_code == 403:
            error_msg.error("PTT ä¸»é é€£ç·šè¢«é˜»æ“‹ï¼ˆ403 Forbiddenï¼‰")
            info_msg.info("å˜—è©¦ä½¿ç”¨ä¸åŒçš„ User-Agent...")
            
            # å˜—è©¦ä¸åŒçš„ User-Agent
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            
            for i, ua in enumerate(user_agents):
                info_msg.info(f"å˜—è©¦ User-Agent {i+1}: {ua[:50]}...")
                session.headers.update({'User-Agent': ua})
                time.sleep(3)
                
                try:
                    test_res = session.get("https://www.ptt.cc/bbs/index.html", timeout=15)
                    if test_res.status_code == 200:
                        info_msg.info(f"User-Agent {i+1} æˆåŠŸï¼")
                        break
                    else:
                        info_msg.info(f"User-Agent {i+1} å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{test_res.status_code}")
                except Exception as e:
                    info_msg.info(f"User-Agent {i+1} é€£ç·šéŒ¯èª¤ï¼š{str(e)}")
            
            # å¦‚æœæ‰€æœ‰ User-Agent éƒ½å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è¨ªå•ç›®æ¨™çœ‹æ¿
            if test_res.status_code != 200:
                info_msg.info("æ‰€æœ‰ User-Agent éƒ½å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è¨ªå•ç›®æ¨™çœ‹æ¿...")
                try:
                    direct_res = session.get(url, timeout=15)
                    if direct_res.status_code == 200:
                        info_msg.info("ç›´æ¥è¨ªå•ç›®æ¨™çœ‹æ¿æˆåŠŸï¼")
                    else:
                        error_msg.error(f"ç›´æ¥è¨ªå•ç›®æ¨™çœ‹æ¿ä¹Ÿå¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{direct_res.status_code}")
                        return pd.DataFrame()
                except Exception as e:
                    error_msg.error(f"ç›´æ¥è¨ªå•ç›®æ¨™çœ‹æ¿é€£ç·šéŒ¯èª¤ï¼š{str(e)}")
                    return pd.DataFrame()
        elif test_res.status_code != 200:
            error_msg.error(f"PTT ä¸»é é€£ç·šå¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{test_res.status_code}")
            return pd.DataFrame()
    except Exception as e:
        error_msg.error(f"PTT ä¸»é é€£ç·šæ¸¬è©¦å¤±æ•—ï¼š{str(e)}")
        return pd.DataFrame()
    
    while page <= max_pages and not stop_crawling:
        progress_msg.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
        try:
            info_msg.info(f"æ­£åœ¨é€£æ¥åˆ°ï¼š{url}")
            res = session.get(url, timeout=15)  # å¢åŠ è¶…æ™‚æ™‚é–“
            info_msg.info(f"ç¬¬ {page} é é€£ç·šç‹€æ…‹ï¼š{res.status_code}")
            
            if res.status_code == 403:
                error_msg.error("PTT æ‹’çµ•é€£ç·šï¼ˆ403 Forbiddenï¼‰ï¼Œå¯èƒ½æ˜¯åçˆ¬èŸ²æ©Ÿåˆ¶")
                info_msg.info("ç­‰å¾… 10 ç§’å¾Œé‡è©¦...")
                time.sleep(10)
                continue
            elif res.status_code == 404:
                error_msg.error(f"çœ‹æ¿ {board} ä¸å­˜åœ¨ï¼ˆ404 Not Foundï¼‰")
                break
            elif res.status_code != 200:
                error_msg.error(f"ç„¡æ³•é€£æ¥åˆ° PTTï¼Œç‹€æ…‹ç¢¼ï¼š{res.status_code}")
                break
                
        except requests.exceptions.Timeout:
            error_msg.error(f"ç¬¬ {page} é é€£ç·šè¶…æ™‚")
            break
        except requests.exceptions.ConnectionError:
            error_msg.error(f"ç¬¬ {page} é é€£ç·šéŒ¯èª¤")
            break
        except Exception as e:
            error_msg.error(f"ç¬¬ {page} é é€£ç·šç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{str(e)}")
            break
            
        # æª¢æŸ¥å›æ‡‰å…§å®¹
        if len(res.text) < 1000:
            error_msg.error(f"ç¬¬ {page} é å›æ‡‰å…§å®¹éçŸ­ï¼Œå¯èƒ½è¢«é˜»æ“‹")
            info_msg.info(f"å›æ‡‰å…§å®¹é•·åº¦ï¼š{len(res.text)} å­—å…ƒ")
            break
            
        soup = BeautifulSoup(res.text, 'html.parser')
        article_items = soup.select('.r-ent')
        info_msg.info(f"ç¬¬ {page} é è§£æçµæœï¼šæ‰¾åˆ° {len(article_items)} å€‹ .r-ent å…ƒç´ ")
        
        if not article_items:
            warning_msg.warning(f"ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
            # æª¢æŸ¥é é¢å…§å®¹ï¼Œçœ‹æ˜¯å¦æœ‰å…¶ä»–å•é¡Œ
            page_title = soup.find('title')
            if page_title:
                info_msg.info(f"é é¢æ¨™é¡Œï¼š{page_title.text}")
            else:
                info_msg.info("ç„¡æ³•æ‰¾åˆ°é é¢æ¨™é¡Œ")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤è¨Šæ¯
            error_div = soup.find('div', class_='error')
            if error_div:
                info_msg.info(f"é é¢éŒ¯èª¤è¨Šæ¯ï¼š{error_div.text}")
            break
            
        info_msg.info(f"ç¬¬ {page} é æ‰¾åˆ° {len(article_items)} ç¯‡æ–‡ç« ï¼ˆæœ€å¤šåªçˆ¬ {max_pages} é ï¼‰")
        page_has_recent_articles = False
        
        for i, article in enumerate(article_items[:3]):  # åªè™•ç†å‰3ç¯‡æ–‡ç« ä½œç‚ºæ¸¬è©¦
            title_element = article.select_one('.title a')
            if not title_element:
                info_msg.info(f"ç¬¬ {i+1} ç¯‡æ–‡ç« æ²’æœ‰æ¨™é¡Œé€£çµ")
                continue
            title = title_element.text.strip()
            info_msg.info(f"ç¬¬ {i+1} ç¯‡æ–‡ç« æ¨™é¡Œï¼š{title}")
            if '[å…¬å‘Š]' in title:
                info_msg.info(f"è·³éå…¬å‘Šæ–‡ç« ï¼š{title}")
                continue
            article_url = base_url + title_element['href']
            author = article.select_one('.meta .author').text.strip() if article.select_one('.meta .author') else "æœªçŸ¥"
            info_msg.info(f"æ–‡ç« ä½œè€…ï¼š{author}ï¼ŒURLï¼š{article_url}")
            
            # é€²å…¥å…§é æŠ“ç™¼æ–‡æ™‚é–“èˆ‡å…§æ–‡
            time.sleep(request_delay)
            try:
                info_msg.info(f"æ­£åœ¨é€£æ¥åˆ°æ–‡ç« å…§é ï¼š{article_url}")
                art_res = session.get(article_url, timeout=15)
                if art_res.status_code != 200:
                    info_msg.info(f"ç„¡æ³•é€£æ¥åˆ°æ–‡ç« å…§é ï¼Œç‹€æ…‹ç¢¼ï¼š{art_res.status_code}")
                    continue
            except Exception as e:
                info_msg.info(f"é€£æ¥åˆ°æ–‡ç« å…§é æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
                continue
                
            art_soup = BeautifulSoup(art_res.text, 'html.parser')
            meta_elements = art_soup.select('.article-meta-value')
            info_msg.info(f"æ–‡ç« å…§é æ‰¾åˆ° {len(meta_elements)} å€‹ meta å…ƒç´ ")
            
            if len(meta_elements) >= 4:
                time_str = meta_elements[3].text.strip()
                info_msg.info(f"æ™‚é–“å­—ä¸²ï¼š{time_str}")
                try:
                    post_time = datetime.datetime.strptime(time_str, '%a %b %d %H:%M:%S %Y')
                    info_msg.info(f"è§£ææ™‚é–“æˆåŠŸï¼š{post_time}")
                except:
                    try:
                        post_time = datetime.datetime.strptime(time_str, '%Y/%m/%d %H:%M:%S')
                        info_msg.info(f"è§£ææ™‚é–“æˆåŠŸï¼ˆç¬¬äºŒç¨®æ ¼å¼ï¼‰ï¼š{post_time}")
                    except:
                        warning_msg.warning(f"ç„¡æ³•è§£ææ™‚é–“æ ¼å¼ï¼š{time_str}")
                        continue
            else:
                info_msg.info("æ–‡ç« å…§é æ²’æœ‰è¶³å¤ çš„ meta å…ƒç´ ")
                continue
                
            if post_time.date() < start:
                info_msg.info(f"é‡åˆ°èˆŠæ–‡ç«  {title}ï¼Œæ™‚é–“ï¼š{post_time.date()}ï¼Œåœæ­¢çˆ¬å–")
                stop_crawling = True
                break
            if not (start <= post_time.date() <= today):
                info_msg.info(f"æ–‡ç«  {title} ä¸åœ¨ç›®æ¨™æ—¥æœŸç¯„åœå…§ï¼š{post_time.date()}")
                continue
            if last_time is not None and post_time <= pd.to_datetime(last_time):
                info_msg.info(f"é‡åˆ°å·²å­˜åœ¨çš„æ–‡ç«  {title}ï¼Œæ™‚é–“ï¼š{post_time}ï¼Œåœæ­¢çˆ¬å–")
                stop_crawling = True
                break
                
            page_has_recent_articles = True
            info_msg.info(f"æ–‡ç«  {title} ç¬¦åˆæ¢ä»¶ï¼Œé–‹å§‹æŠ“å–å…§æ–‡")
            
            # å…§æ–‡
            main_content = art_soup.select_one('#main-content')
            content = ""
            if main_content:
                content_copy = BeautifulSoup(str(main_content), 'html.parser').select_one('#main-content')
                for push in content_copy.select('.push'):
                    push.decompose()
                content_text = content_copy.text
                signature_pos = content_text.find('--')
                if signature_pos > 0:
                    content_text = content_text[:signature_pos].strip()
                content = content_text
                info_msg.info(f"å…§æ–‡é•·åº¦ï¼š{len(content)} å­—å…ƒ")
            else:
                info_msg.info("ç„¡æ³•æ‰¾åˆ°æ–‡ç« å…§æ–‡")
                
            articles.append({
                'timestamp': post_time,
                'content': content,
                'title': title,
                'author': author,
                'board': board
            })
            current_count += 1
            progress_msg.info(f"çˆ¬å– {current_count} ç¯‡ï¼š{title}")
            
        if not page_has_recent_articles and current_count > 0:
            info_msg.info("æœ¬é æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–‡ç« ï¼Œåœæ­¢çˆ¬å–")
            break
        # ç¿»é 
        prev_page = None
        for link in soup.select('.btn-group-paging a'):
            if 'ä¸Šé ' in link.text:
                prev_page = link
                break
        if prev_page and prev_page.has_attr('href'):
            url = base_url + prev_page['href']
            page += 1
            time.sleep(page_delay)
        else:
            info_msg.info("æ²’æœ‰æ›´å¤šé é¢")
            break
    
    # æ¸…é™¤æ‰€æœ‰é€²åº¦è¨Šæ¯ï¼Œåªé¡¯ç¤ºæœ€çµ‚çµæœ
    progress_msg.empty()
    info_msg.empty()
    warning_msg.empty()
    error_msg.empty()
    
    if len(articles) > 0:
        st.success(f"âœ… çˆ¬å–å®Œæˆï¼å…±æ‰¾åˆ° {len(articles)} ç¯‡ç¬¦åˆæ¢ä»¶çš„æ–‡ç« ")
    else:
        st.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–‡ç« ")
    
    # åˆä½µ cache
    if 'articles_df_dict' in st.session_state and board in st.session_state['articles_df_dict'] and not st.session_state['articles_df_dict'][board].empty:
        old_df = st.session_state['articles_df_dict'][board]
        new_df = pd.DataFrame(articles)
        all_df = pd.concat([old_df, new_df], ignore_index=True)
        all_df = all_df.drop_duplicates(subset=['timestamp', 'title', 'author'], keep='last')
        return all_df
    else:
        return pd.DataFrame(articles)